<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Publications | Jake Brawer, Ph.D</title>
<meta name="generator" content="Jekyll v4.4.1" />
<meta property="og:title" content="Publications" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Computer science Ph.D from Yale.Postdoc at CU Boulder.Social Roboticist." />
<meta property="og:description" content="Computer science Ph.D from Yale.Postdoc at CU Boulder.Social Roboticist." />
<link rel="canonical" href="http://localhost:4001/publications/" />
<meta property="og:url" content="http://localhost:4001/publications/" />
<meta property="og:site_name" content="Jake Brawer, Ph.D" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Publications" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","description":"Computer science Ph.D from Yale.Postdoc at CU Boulder.Social Roboticist.","headline":"Publications","url":"http://localhost:4001/publications/"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="stylesheet" href="/assets/css/style.css?v=">
    <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv.min.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper" >
      <header>
        <h1><a href="http://localhost:4001/">Jake Brawer, Ph.D</a></h1>

        
        <div class="headshot-container">
            <img src="/img/jake_bw_circle.jpg" alt="Jake Brawer, Ph.D headshot" class="headshot">
        </div>
        

        <div style="text-align: center; margin-top: 10px;">
        <p><a href="https://mailhide.io/en/e/cb2eTmo6"> Email</a></a> /
        <a href="https://github.com/JakeBrawer">GitHub</a> /
        <a href="https://scholar.google.com/citations?hl=en&user=1WIr3ZsAAAAJ">Scholar</a> /
        <a href="https://www.linkedin.com/in/jake-brawer-947233232/">LinkedIn</a>
        <!-- <a href="https://jakebrawer.com//assets/pdfs/brawer_cv.pdf">CV</a> -->
      </div>
        <p>Computer science Ph.D from Yale.<br>Postdoc at CU Boulder.<br>Social Roboticist.</p>

        

        

        <nav >
        
        
         
        
    <p class="main_nav">
            <a href="/"  >
                Home
            </a>
    </p>
         
        
    <p class="main_nav">
            <a href="/ongoing/"  >
                Current Research
            </a>
    </p>
         
        
    <p class="main_nav">
            <a href="/publications/" class="active" >
                Publications
            </a>
    </p>
         
    <!-- My Cv -->
    <p class="main_nav">
        <a href="http://localhost:4001/assets/pdfs/brawer_cv.pdf">
            CV
        </a>
    </p>

</nav>

        
      </header>


      <section >
              
                












<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/eyes.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>Eyes on the Game: Deciphering Implicit Human Signals to Infer Human Proficiency, Trust, and Intent</h3>
    <p>Nikhil Hulle, St√©phane Aroca-Ouellette, Anthony J Ries, <strong>Jake Brawer</strong>, Katharina Von Der Wense, Alessandro Roncone</p>
    <p><em>33rd IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</em>, 2024</p>
     <a href="https://arxiv.org/pdf/2407.03298">arxiv</a> / 
    
    
    
    
    <p><p>We show that eye gaze data and gameplay data can be used to accurately model a human teammates intentions and beliefs about an AI teammate in the game Overcooked.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/overlay.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>Interactive Policy Shaping for Human-Robot Collaboration with Transparent Matrix Overlays</h3>
    <p><strong>Jake Brawer</strong>, Debasmita Ghose, Kate Candon, Meiying Qin, Alessandro Roncone, Marynel V√°zquez, Brian Scassellati</p>
    <p><em>ACM/IEEE International Conference on Human-Robot Interaction</em>, 2023</p>
    
     <a href="https://scazlab.yale.edu/sites/default/files/files/HRI_2023___Chefbot.pdf">paper</a> / 
     <a href="https://sites.google.com/view/transparent-matrix-overlays/home">website</a> / 
    
    
    <p><p>üèÜ <strong>Best Technical Paper Award</strong>
 This work introduces Transparent Matrix Overlays, a novel framework that allows humans to shape a robot‚Äôs policy during execution using simple verbal commands. The approach aims to enhance both <strong>adaptability</strong> and <strong>explainability</strong> in human-robot collaboration.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/taxonomy.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>Robot Tool Use: A Survey</h3>
    <p>Meiying Qin, <strong>Jake Brawer</strong>, Brian Scassellati</p>
    <p><em>Frontiers in Robotics and AI</em>, 2023</p>
    
     <a href="https://scazlab.yale.edu/sites/default/files/files/robot_tool_use_survey.pdf">paper</a> / 
    
    
    
    <p><p>This survey identifies three critical skills required for robot tool use: perception, manipulation, and high-level cognition. It also introduces a taxonomy inspired by animal tool use literature, laying the foundation for future research and practical guidelines in robotic tool use.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/handover.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>Task-Oriented Robot-to-Human Handovers in Collaborative Tool-Use Tasks</h3>
    <p>Meiying Qin, Jake Brawer, Brian Scassellati</p>
    <p><em>2022 31st IEEE International Conference on Robot and Human Interactive Communication (RO-MAN)</em>, 2022</p>
    
     <a href="https://scazlab.yale.edu/sites/default/files/files/ROMAN%202022%20handovers.pdf">paper</a> / 
    
    
    
    <p><p>This paper introduces a taxonomy of robot-to-human handovers and focuses on <strong>task-oriented handovers</strong>, where the object is passed in a way that facilitates immediate tool use. The proposed method trains the robot via tool-use demonstrations, enabling adaptable handovers even for novel tools and unusual usage contexts.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/tristar.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>Rapidly Learning Generalizable and Robot-Agnostic Tool-Use Skills for a Wide Range of Tasks</h3>
    <p>Meiying Qin*, <strong>Jake Brawer*</strong>, Brian Scassellati</p>
    <p><em>Frontiers in Robotics and AI</em>, 2021</p>
    
     <a href="https://scazlab.yale.edu/sites/default/files/files/Rapidly_Learning_FRONTIERS21.pdf">paper</a> / 
    
    
    
    <p><p><strong>Co-first authors:</strong> Meiying Qin and Jake Brawer contributed equally to this work.
 This paper presents <strong>TRI-STAR</strong>, a framework for teaching robots tool-use skills that generalize across tasks, tools, and robot platforms. With only 20 demonstrations, TRI-STAR learns adaptable and transferable tool-use strategies, enabling robots to solve diverse tasks using novel tools.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/causal_vid.gif" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>A Causal Approach to Tool Affordance Learning</h3>
    <p><strong>Jake Brawer</strong>, Meiying Qin, Brian Scassellati</p>
    <p><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2020</p>
    
     <a href="https://scazlab.yale.edu/sites/default/files/files/IROS2020(5).pdf">paper</a> / 
    
     <a href="https://youtu.be/g6mN4cUFNPw">video</a> / 
    
    <p><p>This paper presents a method for robots to build causal models of tool affordances through observation and self-supervised experimentation. By understanding cause-and-effect relationships, robots can generalize tool-use skills to novel tools and contexts with minimal training.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/ownership.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>That‚Äôs Mine! Learning Ownership Relations and Norms for Robots</h3>
    <p>Zhi-Xuan Tan, <strong>Jake Brawer</strong>, Brian Scassellati</p>
    <p><em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 2019</p>
    
     <a href="https://scazlab.yale.edu/sites/default/files/files/Xuan_AAAI19.pdf">paper</a> / 
    
    
    
    <p><p>This paper presents a system for robots to learn and apply ownership norms through incremental learning and Bayesian inference. By combining rule induction, relation inference, and perception, the system allows robots to reason about ownership and follow social norms during object manipulation.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/grounded.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>Situated Human‚ÄìRobot Collaboration: Predicting Intent from Grounded Natural Language</h3>
    <p><strong>Jake Brawer</strong>, Olivier Mangin, Alessandro Roncone, Sarah Widder, Brian Scassellati</p>
    <p><em>IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</em>, 2018</p>
    
     <a href="https://scazlab.yale.edu/sites/default/files/files/Brawer_IROS18.pdf">paper</a> / 
    
    
    
    <p><p>This paper presents a framework for grounding natural language into robot action selection during collaborative tasks. By maintaining separate models for speech and context, the system incrementally updates its understanding of intent, enabling more fluent human-robot collaboration.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/infant.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>Teaching Language to Deaf Infants with a Robot and a Virtual Human</h3>
    <p>Brian Scassellati, <strong>Jake Brawer</strong>, Katherine Tsui, Setareh Nasihati Gilani, Melissa Malzkuhn, Barbara Manini, Adam Stone, Geo Kartheiser, Arcangelo Merla, Ari Shapiro, David Traum, Laura-Ann Petitto</p>
    <p><em>Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems</em>, 2018</p>
    
     <a href="https://scazlab.yale.edu/sites/default/files/files/pn4384-scassellatiA.pdf">paper</a> / 
    
    
    
    <p><p>This paper presents a multi-agent system combining a robot and virtual human to augment language exposure for deaf infants during critical developmental periods. The system provides socially contingent visual language, addressing the challenge of language acquisition in deaf infants born to hearing parents.</p>
</p>
  </div>
</div>





<div class="research-entry" style="display: flex; align-items: center; margin-bottom: 20px;">
  <!-- Image on the left -->
  <div style="width: 60%; min-width: 120px; text-align: left; padding-right: 15px;">
    
<img src="/img//images/epigenetic.png" alt="project image" style="width:auto; height:auto; max-width:100%;" />

  </div>
  <!-- Text on the right -->
  <div class="research-details" style="width: 75%;">
    <h3>Epigenetic Operators and the Evolution of Physically Embodied Robots</h3>
    <p><strong>Jake Brawer</strong>, Aaron Hill, Ken Livingston, Eric Aaron, Joshua Bongard, John H Long Jr</p>
    <p><em>Frontiers in Robotics and AI</em>, 2017</p>
    
    
    
    
    
    <p><p>This paper investigates how epigenetic operators, alongside standard genetic operators like recombination and mutation, shape the evolution of physically embodied robots. Results show that epigenetic operators can lead to qualitatively different evolutionary outcomes, with some effects reducing adaptability compared to traditional genetic processes.</p>
</p>
  </div>
</div>




              



      </section>
      <footer>
          <!--  -->
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="/assets/js/scale.fix.js"></script>
    
  </body>
</html>
